{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Core Math — Python Companion\n",
    "\n",
    "This notebook runs the same math as the browser simulator, but with **real NumPy float16** instead of JS bit manipulation. Use this alongside the interactive tools to verify the numbers are accurate and to go deeper where the simulator simplifies.\n",
    "\n",
    "Covers:\n",
    "1. FP16 precision — what gets lost and why\n",
    "2. MMA: D = A×B + C with real mixed precision\n",
    "3. Precision error analysis vs FP32 reference\n",
    "4. Binary reduction tree depth\n",
    "5. GEMM tiling\n",
    "6. Transformer FLOP breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams.update({\n",
    "    'axes.facecolor':  '#0f1117',\n",
    "    'figure.facecolor':'#08090b',\n",
    "    'axes.edgecolor':  '#252b3a',\n",
    "    'axes.labelcolor': '#8a9ab0',\n",
    "    'xtick.color':     '#5a6a80',\n",
    "    'ytick.color':     '#5a6a80',\n",
    "    'text.color':      '#dce5f0',\n",
    "    'grid.color':      '#171a23',\n",
    "    'font.family':     'monospace',\n",
    "})\n",
    "\n",
    "ACC  = '#5b8fff'\n",
    "GRN  = '#2ecc7a'\n",
    "ORG  = '#f0853a'\n",
    "RED  = '#e05555'\n",
    "PUR  = '#a67cff'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. FP16 Precision — What Gets Lost\n",
    "\n",
    "NumPy's `float16` is real IEEE 754 half-precision: **1 sign + 5 exponent + 10 mantissa bits**.  \n",
    "The JS simulator approximates this by truncating the FP32 mantissa. Here we use the actual dtype.\n",
    "\n",
    "Key limits:\n",
    "- Max representable value: **65504**\n",
    "- Smallest normal: **~6.1e-5**\n",
    "- Machine epsilon (gap between 1.0 and next value): **~0.00098** (≈ 2⁻¹⁰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FP16 dtype info ===\")\n",
    "info = np.finfo(np.float16)\n",
    "print(f\"  max value   : {info.max}\")\n",
    "print(f\"  min normal  : {info.tiny}\")\n",
    "print(f\"  epsilon     : {info.eps}  (gap between 1.0 and next representable)\")\n",
    "print(f\"  precision   : ~{info.precision} decimal digits\")\n",
    "print()\n",
    "print(\"=== FP32 dtype info ===\")\n",
    "info32 = np.finfo(np.float32)\n",
    "print(f\"  max value   : {info32.max:.3e}\")\n",
    "print(f\"  epsilon     : {info32.eps}\")\n",
    "print(f\"  precision   : ~{info32.precision} decimal digits\")\n",
    "print()\n",
    "\n",
    "# Show where FP16 rounds\n",
    "vals = [0.1, 0.3333, 1.0, 1.001, 100.0, 1000.0, 10000.0]\n",
    "print(f\"{'Value':>10}  {'FP32':>12}  {'FP16':>12}  {'Abs Error':>12}\")\n",
    "print(\"-\" * 52)\n",
    "for v in vals:\n",
    "    f32 = np.float32(v)\n",
    "    f16 = np.float16(v)\n",
    "    err = abs(float(f32) - float(f16))\n",
    "    print(f\"{v:>10}  {float(f32):>12.6f}  {float(f16):>12.6f}  {err:>12.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the density of representable FP16 values\n",
    "# (they're not evenly spaced — denser near 0, sparser at large values)\n",
    "ranges = [\n",
    "    (0.0, 2.0, 'Dense near 0'),\n",
    "    (1.0, 4.0, 'Mid range'),\n",
    "    (512.0, 1024.0, 'Large values'),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 3))\n",
    "fig.suptitle('FP16 Representable Value Density', color='#dce5f0', fontsize=11)\n",
    "\n",
    "for ax, (lo, hi, title) in zip(axes, ranges):\n",
    "    pts = np.linspace(lo, hi, 2000, dtype=np.float32)\n",
    "    pts_f16 = pts.astype(np.float16).astype(np.float32)\n",
    "    unique_vals = np.unique(pts_f16)\n",
    "    ax.scatter(unique_vals, np.ones_like(unique_vals), s=2, color=ACC, alpha=0.7)\n",
    "    ax.set_title(title, fontsize=9, color='#8a9ab0')\n",
    "    ax.set_xlabel('value', fontsize=8)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim(lo, hi)\n",
    "    ax.annotate(f'{len(unique_vals)} distinct values', xy=(0.5, 0.75),\n",
    "                xycoords='axes fraction', ha='center', fontsize=8, color=GRN)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. MMA: D = A×B + C\n",
    "\n",
    "The tensor core's one operation. Inputs A, B are FP16. C (accumulator) and output D are FP32.  \n",
    "This mixed-precision path is what lets you compute fast (FP16 multiplies) while accumulating accurately (FP32 adds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def make_matrix_fp16(n=4, scale=3.0):\n",
    "    \"\"\"Random FP16 matrix, values in [-scale, scale].\"\"\"\n",
    "    return (rng.uniform(-scale, scale, (n, n))).astype(np.float16)\n",
    "\n",
    "def mma(A_fp16, B_fp16, C_fp32):\n",
    "    \"\"\"\n",
    "    Simulate tensor core MMA: D = A @ B + C\n",
    "    \n",
    "    Hardware path:\n",
    "      1. Load A, B as FP16\n",
    "      2. Each element multiply: FP16 × FP16 → promoted to FP32\n",
    "      3. Accumulate products in FP32 (reduction tree)\n",
    "      4. Add FP32 accumulator C\n",
    "      5. Output D in FP32\n",
    "    \"\"\"\n",
    "    # Cast up to FP32 for accumulation — mirrors hardware upcast after multiply\n",
    "    A = A_fp16.astype(np.float32)\n",
    "    B = B_fp16.astype(np.float32)\n",
    "    return A @ B + C_fp32\n",
    "\n",
    "def mma_fp32_reference(n=4, scale=3.0):\n",
    "    \"\"\"Pure FP32 reference — same values but no FP16 rounding.\"\"\"\n",
    "    A = rng.uniform(-scale, scale, (n, n)).astype(np.float32)\n",
    "    B = rng.uniform(-scale, scale, (n, n)).astype(np.float32)\n",
    "    C = rng.uniform(-0.5, 0.5, (n, n)).astype(np.float32)\n",
    "    return A @ B + C\n",
    "\n",
    "# Run one MMA\n",
    "A = make_matrix_fp16()\n",
    "B = make_matrix_fp16()\n",
    "C = rng.uniform(-0.5, 0.5, (4, 4)).astype(np.float32)\n",
    "\n",
    "D = mma(A, B, C)\n",
    "\n",
    "print(\"A (FP16):\")\n",
    "print(A)\n",
    "print(f\"\\nB (FP16):\")\n",
    "print(B)\n",
    "print(f\"\\nC accumulator (FP32):\")\n",
    "print(np.round(C, 4))\n",
    "print(f\"\\nD = A×B + C (FP32 output):\")\n",
    "print(np.round(D, 4))\n",
    "print(f\"\\nD dtype: {D.dtype}  ← always FP32 out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Precision Error Analysis\n",
    "\n",
    "How much error does the FP16 input path introduce vs a pure FP32 computation?  \n",
    "We run many MMAs, collect the max absolute error per run, and look at the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_error_trial(scale=3.0, n=4):\n",
    "    \"\"\"One trial: compare FP16-input MMA vs pure FP32 MMA on the same values.\"\"\"\n",
    "    # Generate in FP32, then cast A and B to FP16 (introducing rounding)\n",
    "    A_fp32 = rng.uniform(-scale, scale, (n, n)).astype(np.float32)\n",
    "    B_fp32 = rng.uniform(-scale, scale, (n, n)).astype(np.float32)\n",
    "    C      = rng.uniform(-0.5, 0.5, (n, n)).astype(np.float32)\n",
    "\n",
    "    A_fp16 = A_fp32.astype(np.float16)\n",
    "    B_fp16 = B_fp32.astype(np.float16)\n",
    "\n",
    "    D_tc  = mma(A_fp16, B_fp16, C)        # tensor core path: FP16 inputs\n",
    "    D_ref = A_fp32 @ B_fp32 + C           # reference: pure FP32\n",
    "\n",
    "    return np.max(np.abs(D_tc - D_ref))\n",
    "\n",
    "N_TRIALS = 5000\n",
    "errors = [run_error_trial() for _ in range(N_TRIALS)]\n",
    "errors = np.array(errors)\n",
    "\n",
    "print(f\"Over {N_TRIALS} random 4×4 MMAs (FP16 inputs, scale ±3):\")\n",
    "print(f\"  mean max error  : {errors.mean():.4f}\")\n",
    "print(f\"  median max error: {np.median(errors):.4f}\")\n",
    "print(f\"  p99 max error   : {np.percentile(errors, 99):.4f}\")\n",
    "print(f\"  worst case      : {errors.max():.4f}\")\n",
    "print()\n",
    "print(\"Compare to FP16 epsilon (~0.001) — error scales with magnitude of values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "# Error distribution\n",
    "ax = axes[0]\n",
    "ax.hist(errors, bins=60, color=ACC, alpha=0.8, edgecolor='none')\n",
    "ax.axvline(errors.mean(), color=GRN, lw=1.5, ls='--', label=f'mean={errors.mean():.3f}')\n",
    "ax.axvline(np.percentile(errors, 99), color=ORG, lw=1.5, ls='--', label=f'p99={np.percentile(errors,99):.3f}')\n",
    "ax.set_xlabel('Max absolute error per MMA')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('FP16-path vs FP32 reference error\\n(4×4 MMA, 5000 trials)', fontsize=10)\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Error vs input scale\n",
    "ax = axes[1]\n",
    "scales = np.linspace(0.1, 10, 40)\n",
    "mean_errors = [np.mean([run_error_trial(scale=s) for _ in range(200)]) for s in scales]\n",
    "ax.plot(scales, mean_errors, color=ORG, lw=2)\n",
    "ax.set_xlabel('Input value scale (±s)')\n",
    "ax.set_ylabel('Mean max error')\n",
    "ax.set_title('Error grows with input magnitude\\n(FP16 has fixed relative precision)', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nKey insight: FP16 error is proportional to value magnitude (relative precision is fixed).\")\n",
    "print(\"This is why weight initialization and gradient scaling matter in mixed-precision training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Binary Reduction Tree\n",
    "\n",
    "Inside the tensor core, N multiplier outputs are summed using a binary adder tree.  \n",
    "Depth = ⌈log₂(N)⌉ — this is the critical path that limits clock speed, not N itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tree_depth(n):\n",
    "    return math.ceil(math.log2(n))\n",
    "\n",
    "def simulate_reduction_tree(values, input_bits=16):\n",
    "    \"\"\"\n",
    "    Simulate a binary reduction tree.\n",
    "    Returns: (result, depth, bit_widths_per_level)\n",
    "    Each level of adds needs 1 extra bit to hold carry.\n",
    "    \"\"\"\n",
    "    level = list(values)\n",
    "    depth = 0\n",
    "    bit_widths = [input_bits]\n",
    "    while len(level) > 1:\n",
    "        next_level = []\n",
    "        for i in range(0, len(level) - 1, 2):\n",
    "            next_level.append(level[i] + level[i+1])\n",
    "        if len(level) % 2 == 1:\n",
    "            next_level.append(level[-1])  # pass odd one through\n",
    "        level = next_level\n",
    "        depth += 1\n",
    "        bit_widths.append(input_bits + depth)  # grows by 1 bit per level\n",
    "    return level[0], depth, bit_widths\n",
    "\n",
    "# Compare tree depth vs sequential depth\n",
    "ns = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "df = pd.DataFrame({\n",
    "    'N (multipliers)': ns,\n",
    "    'Tree depth (log₂N)': [tree_depth(n) for n in ns],\n",
    "    'Sequential depth (N-1)': [n-1 for n in ns],\n",
    "    'Speedup factor': [round((n-1)/tree_depth(n), 1) for n in ns],\n",
    "    'Final bit width (FP16 in)': [16 + tree_depth(n) for n in ns],\n",
    "})\n",
    "print(df.to_string(index=False))\n",
    "print()\n",
    "print(\"Full Volta tensor core: N=64 multipliers → depth=6 adds on critical path\")\n",
    "print(\"(vs 63 sequential adds — 10.5× shallower pipeline)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "ns_plot = range(2, 257)\n",
    "tree_depths   = [tree_depth(n) for n in ns_plot]\n",
    "seq_depths    = [n-1 for n in ns_plot]\n",
    "\n",
    "ax.plot(list(ns_plot), tree_depths, color=ACC, lw=2.5, label='Tree depth: ⌈log₂(N)⌉')\n",
    "ax.plot(list(ns_plot), seq_depths,  color=RED, lw=1.5, ls='--', label='Sequential: N-1')\n",
    "\n",
    "# Annotate the Volta TC point\n",
    "ax.axvline(64, color=GRN, lw=1, ls=':', alpha=0.8)\n",
    "ax.annotate('Volta TC\\nN=64, depth=6',\n",
    "            xy=(64, tree_depth(64)), xytext=(80, 20),\n",
    "            color=GRN, fontsize=9,\n",
    "            arrowprops=dict(arrowstyle='->', color=GRN, lw=1))\n",
    "\n",
    "ax.set_xlabel('N (number of multipliers)')\n",
    "ax.set_ylabel('Adder depth (critical path)')\n",
    "ax.set_title('Binary Reduction Tree: log₂(N) depth vs sequential N-1', fontsize=11)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(2, 256)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. GEMM Tiling\n",
    "\n",
    "A large GEMM (e.g. 1024×1024) is broken into 4×4 tiles that fit in on-chip SRAM.  \n",
    "Each tile executes one tensor core MMA. The outer loops are software-managed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiled_gemm_fp16(A_fp16, B_fp16, tile=4):\n",
    "    \"\"\"\n",
    "    Tiled GEMM simulating how cuBLAS dispatches tensor core MMAs.\n",
    "    A: [M, K] FP16\n",
    "    B: [K, N] FP16\n",
    "    Returns D: [M, N] FP32\n",
    "    \"\"\"\n",
    "    M, K = A_fp16.shape\n",
    "    K2, N = B_fp16.shape\n",
    "    assert K == K2\n",
    "    assert M % tile == 0 and K % tile == 0 and N % tile == 0\n",
    "\n",
    "    D = np.zeros((M, N), dtype=np.float32)  # FP32 output\n",
    "    mma_count = 0\n",
    "\n",
    "    for tm in range(M // tile):\n",
    "        for tn in range(N // tile):\n",
    "            C_tile = np.zeros((tile, tile), dtype=np.float32)  # FP32 accumulator\n",
    "            for tk in range(K // tile):\n",
    "                A_tile = A_fp16[tm*tile:(tm+1)*tile, tk*tile:(tk+1)*tile]\n",
    "                B_tile = B_fp16[tk*tile:(tk+1)*tile, tn*tile:(tn+1)*tile]\n",
    "                # One tensor core MMA: FP16 inputs, FP32 accumulate\n",
    "                C_tile = mma(A_tile, B_tile, C_tile)\n",
    "                mma_count += 1\n",
    "            D[tm*tile:(tm+1)*tile, tn*tile:(tn+1)*tile] = C_tile\n",
    "\n",
    "    return D, mma_count\n",
    "\n",
    "# Test on a 16×16 matrix\n",
    "M = N = K = 16\n",
    "A = rng.uniform(-2, 2, (M, K)).astype(np.float16)\n",
    "B = rng.uniform(-2, 2, (K, N)).astype(np.float16)\n",
    "\n",
    "D_tiled, n_mmas = tiled_gemm_fp16(A, B, tile=4)\n",
    "D_ref = A.astype(np.float32) @ B.astype(np.float32)\n",
    "\n",
    "flops = 2 * M * N * K\n",
    "print(f\"GEMM {M}×{K} × {K}×{N}:\")\n",
    "print(f\"  MMA calls     : {n_mmas}  (= {M//4} × {N//4} × {K//4} tiles)\")\n",
    "print(f\"  FLOPs         : {flops:,}  (= 2MNK)\")\n",
    "print(f\"  FLOPs/MMA     : {flops // n_mmas}  (= 2×4³ = 128 per 4×4×4 MMA)\")\n",
    "print(f\"  Max abs error : {np.max(np.abs(D_tiled - D_ref)):.5f}\")\n",
    "print()\n",
    "\n",
    "sizes = [16, 32, 64, 128, 256, 512, 1024]\n",
    "print(f\"{'M=N=K':>8}  {'MMAs':>8}  {'FLOPs':>12}  {'Bytes moved (FP16)':>20}  {'Arith Intensity':>16}\")\n",
    "print(\"-\" * 72)\n",
    "for s in sizes:\n",
    "    mmas   = (s//4)**3\n",
    "    flops  = 2 * s**3\n",
    "    bytes_ = 2 * s**2 * 2   # A + B, FP16 = 2 bytes each\n",
    "    ai     = flops / bytes_\n",
    "    print(f\"{s:>8}  {mmas:>8,}  {flops:>12,}  {bytes_:>20,}  {ai:>15.1f}\")\n",
    "\n",
    "print(\"\\nArithmetic intensity (FLOPs/byte) grows with matrix size — large GEMMs are compute-bound.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Transformer FLOP Breakdown\n",
    "\n",
    "For a transformer layer with `d_model` and sequence length `S`, almost all FLOPs are GEMMs and therefore tensor core workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    'GPT-2 (1.5B)':     dict(d=1600,  layers=48,  dff=6400,  heads=25),\n",
    "    'GPT-3 (175B)':     dict(d=12288, layers=96,  dff=49152, heads=96),\n",
    "    'LLaMA-3 (70B)':    dict(d=8192,  layers=80,  dff=28672, heads=64),\n",
    "    'GPT-4 class':      dict(d=18432, layers=120, dff=73728, heads=128),\n",
    "}\n",
    "\n",
    "GPUS = {\n",
    "    'V100':  125e12,   # 125 TFLOPS FP16\n",
    "    'A100':  312e12,\n",
    "    'H100':  989e12,\n",
    "}\n",
    "\n",
    "def transformer_flops(d, layers, dff, S):\n",
    "    \"\"\"FLOPs for one forward pass. All per-layer values × L layers.\"\"\"\n",
    "    per_layer = {\n",
    "        'QKᵀ  (TC)':      2 * S * d * d,        # attention: Q·Kᵀ\n",
    "        '·V   (TC)':      2 * S * d * d,        # attention: softmax·V\n",
    "        'OutP (TC)':      2 * S * d * d,        # output projection\n",
    "        'MLP↑ (TC)':      2 * S * d * dff,      # MLP up-project\n",
    "        'MLP↓ (TC)':      2 * S * dff * d,      # MLP down-project\n",
    "        'Softmax':        S * S,                 # elementwise — NOT TC\n",
    "        'LayerNorm':      10 * S * d,            # elementwise — NOT TC\n",
    "    }\n",
    "    return {k: v * layers for k, v in per_layer.items()}\n",
    "\n",
    "SEQ_LEN = 2048\n",
    "\n",
    "rows = []\n",
    "for model_name, cfg in MODELS.items():\n",
    "    flops = transformer_flops(cfg['d'], cfg['layers'], cfg['dff'], SEQ_LEN)\n",
    "    total = sum(flops.values())\n",
    "    tc    = sum(v for k, v in flops.items() if 'TC' in k)\n",
    "    rows.append({\n",
    "        'Model': model_name,\n",
    "        'Total FLOPs': total,\n",
    "        'TC FLOPs %': f\"{tc/total*100:.1f}%\",\n",
    "        'V100 time': f\"{total/GPUS['V100']*1000:.1f} ms\",\n",
    "        'A100 time': f\"{total/GPUS['A100']*1000:.1f} ms\",\n",
    "        'H100 time': f\"{total/GPUS['H100']*1000:.1f} ms\",\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).set_index('Model')\n",
    "df['Total FLOPs'] = df['Total FLOPs'].apply(lambda x: f\"{x:.2e}\")\n",
    "print(f\"Transformer FLOPs — seq_len={SEQ_LEN} (single forward pass, peak TFLOPS, no batching)\")\n",
    "print()\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked bar: FLOP breakdown by operation for each model\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "op_colors = {\n",
    "    'QKᵀ  (TC)': ACC,\n",
    "    '·V   (TC)': '#4a7ce8',\n",
    "    'OutP (TC)': '#3d6cd0',\n",
    "    'MLP↑ (TC)': GRN,\n",
    "    'MLP↓ (TC)': '#26b86e',\n",
    "    'Softmax':   ORG,\n",
    "    'LayerNorm': '#5a6a80',\n",
    "}\n",
    "\n",
    "model_names = list(MODELS.keys())\n",
    "all_flops   = {m: transformer_flops(**{k:v for k,v in cfg.items()}, S=SEQ_LEN)\n",
    "               for m, cfg in MODELS.items()}\n",
    "op_names    = list(op_colors.keys())\n",
    "\n",
    "# Left: absolute FLOPs\n",
    "ax = axes[0]\n",
    "bottoms = np.zeros(len(model_names))\n",
    "for op in op_names:\n",
    "    vals = np.array([all_flops[m][op] for m in model_names])\n",
    "    ax.bar(model_names, vals / 1e12, bottom=bottoms / 1e12,\n",
    "           color=op_colors[op], label=op.strip(), width=0.6)\n",
    "    bottoms += vals\n",
    "ax.set_ylabel('FLOPs (TFLOPS equivalent)')\n",
    "ax.set_title(f'Total FLOPs by operation\\n(seq_len={SEQ_LEN})', fontsize=10)\n",
    "ax.tick_params(axis='x', labelrotation=15)\n",
    "ax.legend(fontsize=8, loc='upper left')\n",
    "ax.grid(True, alpha=0.2, axis='y')\n",
    "\n",
    "# Right: percentage breakdown\n",
    "ax = axes[1]\n",
    "bottoms = np.zeros(len(model_names))\n",
    "for op in op_names:\n",
    "    totals = np.array([sum(all_flops[m].values()) for m in model_names])\n",
    "    vals   = np.array([all_flops[m][op] for m in model_names])\n",
    "    pcts   = vals / totals * 100\n",
    "    ax.bar(model_names, pcts, bottom=bottoms,\n",
    "           color=op_colors[op], label=op.strip(), width=0.6)\n",
    "    bottoms += pcts\n",
    "ax.set_ylabel('% of total FLOPs')\n",
    "ax.set_title('FLOP distribution (% of total)\\nBlue/green = tensor core, orange/gray = elementwise', fontsize=10)\n",
    "ax.tick_params(axis='x', labelrotation=15)\n",
    "ax.set_ylim(0, 105)\n",
    "ax.axhline(100, color='#252b3a', lw=1)\n",
    "ax.grid(True, alpha=0.2, axis='y')\n",
    "\n",
    "# Annotate TC % on right chart\n",
    "for i, m in enumerate(model_names):\n",
    "    total = sum(all_flops[m].values())\n",
    "    tc    = sum(v for k, v in all_flops[m].items() if 'TC' in k)\n",
    "    ax.text(i, 102, f\"{tc/total*100:.0f}% TC\", ha='center', fontsize=8, color=GRN)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Roofline Model\n",
    "\n",
    "Two limits to GPU throughput: **peak compute** and **memory bandwidth**.  \n",
    "The roofline shows which constraint applies given a workload's arithmetic intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_specs = {\n",
    "    'V100': dict(peak_tflops=125,  bw_gbps=900,   color='#5a6a80'),\n",
    "    'A100': dict(peak_tflops=312,  bw_gbps=2000,  color=ACC),\n",
    "    'H100': dict(peak_tflops=989,  bw_gbps=3350,  color=GRN),\n",
    "}\n",
    "\n",
    "# Arithmetic intensity = FLOPs / bytes\n",
    "ai = np.logspace(-2, 3, 500)  # 0.01 to 1000 FLOPs/byte\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for gpu_name, spec in gpu_specs.items():\n",
    "    peak  = spec['peak_tflops'] * 1e12   # FLOPs/s\n",
    "    bw    = spec['bw_gbps'] * 1e9        # bytes/s\n",
    "    roof  = np.minimum(peak, ai * bw)    # roofline: min(compute bound, memory bound)\n",
    "    ax.plot(ai, roof / 1e12, color=spec['color'], lw=2, label=f\"{gpu_name} ({spec['peak_tflops']} TFLOPS, {spec['bw_gbps']} GB/s)\")\n",
    "    ridge = peak / bw  # arithmetic intensity at the \"ridge point\"\n",
    "    ax.axvline(ridge, color=spec['color'], lw=0.8, ls=':', alpha=0.6)\n",
    "\n",
    "# Annotate workload operating points\n",
    "workloads = [\n",
    "    ('LLM inference\\n(batch=1)', 0.05, 50, RED, '^'),\n",
    "    ('LLM inference\\n(batch=32)', 1.5, 180, ORG, '^'),\n",
    "    ('LLM training\\n(large batch)', 100, 600, GRN, 's'),\n",
    "]\n",
    "for label, x, y, color, marker in workloads:\n",
    "    ax.scatter([x], [y], color=color, s=80, zorder=5, marker=marker)\n",
    "    ax.annotate(label, (x, y), textcoords='offset points', xytext=(10, 5),\n",
    "                fontsize=8, color=color)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Arithmetic Intensity (FLOPs / byte)')\n",
    "ax.set_ylabel('Attainable throughput (TFLOPS)')\n",
    "ax.set_title('Roofline Model — V100 / A100 / H100', fontsize=12)\n",
    "ax.legend(fontsize=9, loc='upper left')\n",
    "ax.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "ax.annotate('← memory bound', xy=(0.02, 5), fontsize=9, color=ORG, alpha=0.8)\n",
    "ax.annotate('compute bound →', xy=(200, 5), fontsize=9, color=GRN, alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Ridge points (memory bound → compute bound transition):\")\n",
    "for gpu_name, spec in gpu_specs.items():\n",
    "    ridge = (spec['peak_tflops'] * 1e12) / (spec['bw_gbps'] * 1e9)\n",
    "    print(f\"  {gpu_name}: {ridge:.0f} FLOPs/byte\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
